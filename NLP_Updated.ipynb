{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "176b6160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\" items, beginning with various punctuation symbols and continuingwith words starting with A. All capitalized words precede lowercase words. We dis-cover the size of the vocabulary indirectly, by asking for the number of items in the set,and again we can use len to obtain this number\"\"\"\n",
    "\n",
    "text_split=text.split(\" \")\n",
    "len(text_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91bad7",
   "metadata": {},
   "source": [
    "# Text pre-processing :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b666b02",
   "metadata": {},
   "source": [
    "## Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d43443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Words Tokenizing \"\"\" : Breaks into word-tokens\n",
    "import nltk\n",
    "word_token=nltk.word_tokenize(text)\n",
    "word_token\n",
    "\n",
    "\n",
    "\"\"\" Sentence Tokenizing \"\"\" : Breaks into word-tokens\n",
    "import nltk\n",
    "sentence_token=nltk.sent_tokenize(text)\n",
    "sentence_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780e130",
   "metadata": {},
   "source": [
    "### Python String Attributes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb110ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review\"]=data[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da04116",
   "metadata": {},
   "source": [
    "### Pandas String Attributes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c361b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c36b92",
   "metadata": {},
   "source": [
    "## Unique words & Sorted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cca9da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T08:14:52.409206Z",
     "start_time": "2021-10-05T08:14:52.369204Z"
    }
   },
   "outputs": [],
   "source": [
    "set(tokenized_word)             # Unique Set\n",
    "sorted(set(tokenized_word))     # Unique Set in sorted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fbc58c",
   "metadata": {},
   "source": [
    "## Remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tag(text):\n",
    "    remove_html_tag=re.compile(r'<.*?>')\n",
    "    return re.sub(remove_html_tag,'',text)\n",
    "\n",
    "data['review']=data['review'].apply(remove_html_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481ad1c",
   "metadata": {},
   "source": [
    "## Remove Puncuation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75316dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    review=[letters.lower() for letters in text if letters not in string.punctuation]\n",
    "    review=''.join(review)\n",
    "    return review\n",
    "\n",
    "data['review']=data['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355fc20",
   "metadata": {},
   "source": [
    "## Removing Stop Words :\n",
    " * ``Common words that generally don't contribute to meaning of sentence``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\"\"\" list of Stopword-supported Language \"\"\"\n",
    "stopwords.fileids()  \n",
    "\n",
    "\"\"\" Language Specific  Stopwords \"\"\"\n",
    "stop_words=stopwords.words(\"english\")\n",
    "stop_words    # list of 'english' Stop-words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4af404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words=stopwords.words('english')\n",
    "    review=[words for words in sentence.split() if words not in stop_words]\n",
    "    review=' '.join(review)\n",
    "    return review\n",
    "\n",
    "data['review']=data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f314c63",
   "metadata": {},
   "source": [
    "### Stemming :\n",
    "* `Extract base-form of words` \n",
    "\n",
    "ex: writing | written | wrote -> write\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58efec6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'language'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31036/338143886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# SnowballStemmer: writing -> write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mstrmmerized_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'language'"
     ]
    }
   ],
   "source": [
    "\"\"\" PorterStemmer \"\"\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "\n",
    "strmmerized_word=[ps.stem(w) for w in word_token]\n",
    "strmmerized_word\n",
    "______________________________________________________________________________________________________________________________________________________________-\n",
    "\n",
    "\"\"\" LancasterStemmer \"\"\" \n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc=LancasterStemmer()\n",
    "\n",
    "strmmerized_word=[lc.stem(w) for w in word_token] \n",
    "strmmerized_word\n",
    "______________________________________________________________________________________________________________________________________________________________-\n",
    "\n",
    "\"\"\" SnowballStemmer \"\"\"\n",
    "@ SnowballStemmer(language,ignore_stopwords=False)\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer      \n",
    "sb=SnowballStemmer(\"english\")                   \n",
    "# supported: Arabic,Danish,Dutch,English,Finnish,French,German,Hungarian,Italian,Norwegian,Portuguese,Romanian,Russian,Spanish,Swedish\n",
    "\n",
    "strmmerized_word=[sb.stem(w) for w in word_token] \n",
    "strmmerized_word\n",
    "______________________________________________________________________________________________________________________________________________________________-\n",
    "\n",
    "\"\"\" RegexpStemmer \"\"\"\n",
    "@ RegexpStemmer(regexp,min=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer \n",
    "ps=PorterStemmer() \n",
    "\n",
    "data['review']=data['review'].apply(ps.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f48882",
   "metadata": {},
   "source": [
    "### Lemmatizing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a37399a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T08:29:01.286298Z",
     "start_time": "2021-10-05T08:29:01.233253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from nltk.stem import WordNetLemmatizer\\nlm=WordNetLemmatizer()\\n\\nlemmatized_word=[lm.lemmatize(w) for w in word_token] \\nlemmatized_word\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm=WordNetLemmatizer()\n",
    "\n",
    "lemmatized_word=[lm.lemmatize(w) for w in word_token] \n",
    "lemmatized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm=WordNetLemmatizer()\n",
    "\n",
    "data['review']=data['review'].apply(lm.lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495a06e",
   "metadata": {},
   "source": [
    "### Tagging Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b191e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Text :\n",
    "strip , split , replace \n",
    "\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "\n",
    "#\n",
    "\n",
    "## Tagging Parts of Speech(Using Pre-trained tagger)\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_tagged=pos_tag(word_tokenize(passage1))  # Use pre-trained part of speech tagger\n",
    "text_tagged   # word & Part of Speech\n",
    "\n",
    "# Filter words according to Parts of Speech\n",
    "tagged=[i for i,tag in text_tagged if tag in['NN','NNS','NNP','NNPS']]\n",
    "tagged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## training own Parts of Speech Tagger\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "# Get some text from the Brown Corpus, broken into sentences\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "# Split into 4000 sentences for training and 623 for testing\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "# Create backoff tagger\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "# Show accuracy\n",
    "trigram.evaluate(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0fa8b3",
   "metadata": {},
   "source": [
    "## Before & After Preprocessing Sentence-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word count Before Preprocessing\n",
    "data[\"pre_process_len\"]=data[\"review\"].str.len()\n",
    "\n",
    "## Word count After Preprocessing\n",
    "data[\"post_process_len\"]=data[\"review\"].str.len()      # sentence length After preprocessing \n",
    "data[\"reduction_percent\"]=round((data[\"post_process_len\"]/data[\"pre_process_len\"])*100)  # % of length reduction Afetr preprocessing \n",
    "print(data[\"reduction_percent\"].mean())      # Reduction Avg. = 63 %\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aacd3c",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b34c2",
   "metadata": {},
   "source": [
    "## Bag of Words (Count Vectorizer) :\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8135e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ CountVectorizer(*,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=<class 'numpy.int64'>)\n",
    " * input:(filename,file,content) default='content'\n",
    "    If filename: Expected to be list of filenames that need reading to fetch raw content to analyze\n",
    "    If file: Sequence must have read method(file-like object) that is called to fetch bytes in memory\n",
    "    If content: Expected to be sequence of items that can be of type string|byte\n",
    "        \n",
    " * encoding:(str)  default='utf-8'\n",
    "    If 'bytes|files' to be analyze, encoding is used to decode\n",
    "\n",
    "decode_error('strict','ignore','replace') default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. By default, it is ‘strict’, meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.\n",
    "\n",
    "strip_accents{‘ascii’, ‘unicode’}, default=None\n",
    "Remove accents and perform other character normalization during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have an direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any characters. None (default) does nothing.\n",
    "\n",
    "Both ‘ascii’ and ‘unicode’ use NFKD normalization from unicodedata.normalize.\n",
    "\n",
    " * lowercase:(bool) Convert all characters to lowercase before tokenizing  (default=True)\n",
    "\n",
    " * preprocessor:(callable)   default=None\n",
    "      Override preprocessing stages (strip_accents & lowercase) while preserving tokenizing & n-grams generation steps\n",
    "      Only applies if analyzer is not callable\n",
    " \n",
    " * tokenizer:(callable) default=None\n",
    "      Override string tokenization step while preserving preprocessing & n-grams generation steps\n",
    "      Only applies if analyzer=='word'\n",
    "\n",
    " * stop_words:'english',list  default=None\n",
    "      If 'english':built-in stop word list for English is used.\n",
    "      If list: Assumed to contain stop words, all of which will be removed from resulting tokens\n",
    "      Only applies if analyzer=='word'.\n",
    "      If None: no stop words will be used. \n",
    "      max_df: can be set to value in range (0.7,1.0) to automatically detect & filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    " * token_pattern:str, default=r”(?u)\\b\\w\\w+\\b”\n",
    "Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "If there is a capturing group in token_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.\n",
    "\n",
    "ngram_rangetuple (min_n, max_n), default=(1, 1)\n",
    "The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable.\n",
    "\n",
    "analyzer{‘word’, ‘char’, ‘char_wb’} or callable, default=’word’\n",
    "Whether the feature should be made of word n-gram or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n",
    "\n",
    "Changed in version 0.21.\n",
    "\n",
    "Since v0.21, if input is filename or file, the data is first read from the file and then passed to the given callable analyzer.\n",
    "\n",
    "max_dffloat in range [0.0, 1.0] or int, default=1.0\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_dffloat in range [0.0, 1.0] or int, default=1\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_featuresint, default=None\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabularyMapping or iterable, default=None\n",
    "Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index.\n",
    "\n",
    "binarybool, default=False\n",
    "If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n",
    "\n",
    "dtypetype, default=np.int64\n",
    "Type of the matrix returned by fit_transform() or transform().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3aebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentences=nltk.sent_tokenize(passage1)    # word-tokenizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "corpus=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    extracted_word=re.sub('[^a-zA-Z]',' ',sentences[i]) # Remove Punctuations \n",
    "    lower_word=extracted_word.lower()                   # Make Lowercase\n",
    "    split_word=lower_word.split()                       # split into words\n",
    "    extract_stop_word=[lemmatizer.lemmatize(word) for word in split_word if not word in set(stopwords.words('english'))]\n",
    "    join_word=' '.join(extract_stop_word)               # Convert to Sentense\n",
    "    corpus.append(join_word)                            # Append to corpus[]\n",
    "\n",
    "# Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "c_vectorizer=CountVectorizer(max_features=1500)\n",
    "bag_of_words=c_vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "c_vectorizer.get_feature_names()    # Return feature names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704259e",
   "metadata": {},
   "source": [
    "## Weighting Word Importance(Using TFIDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b503abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentences=nltk.sent_tokenize(passage1)    # word-tokenizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "corpus=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    extracted_word=re.sub('[^a-zA-Z]',' ',sentences[i]) # Remove Punctuations \n",
    "    lower_word=extracted_word.lower()                   # Make Lowercase\n",
    "    split_word=lower_word.split()                       # split into words\n",
    "    extract_stop_word=[lemmatizer.lemmatize(word) for word in split_word if not word in set(stopwords.words('english'))]\n",
    "    join_word=' '.join(extract_stop_word)               # Convert to Sentense\n",
    "    corpus.append(join_word)                            # Append to corpus[]\n",
    "    \n",
    "    \n",
    "# TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()\n",
    "weighted_word_importance=cv.fit_transform(corpus).toarray()\n",
    "weighted_word_importance\n",
    "\n",
    "tfidf.vocabulary_    # Return Feature nmaes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75cb49a",
   "metadata": {},
   "source": [
    "## Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "text=re.sub(r'\\[[0-9]*\\]',' ',passage1)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "text=text.lower()\n",
    "text=re.sub(r'\\d',' ',text)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences=nltk.sent_tokenize(text)                                    # to sentences \n",
    "sentences=[nltk.word_tokenize(sentence) for sentence in sentences]    # \n",
    "for i in range(len(sentences)):\n",
    "    sentences[i]=[word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n",
    "# Training the Word2Vec model\n",
    "model=Word2Vec(sentences,min_count=1)\n",
    "words=model.wv.vocab\n",
    "vector=model.wv['learning']              # Finding Word Vectors\n",
    "similar=model.wv.most_similar('neural')  # Most similar words\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2dd8c7",
   "metadata": {},
   "source": [
    "## With TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efb837d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T17:00:25.241943Z",
     "start_time": "2021-09-20T17:00:25.226700Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'i love my dog so watch out',\n",
    "    'I, love my cat see',\n",
    "    'You love my dog! seen it a while'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91efd858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T16:58:32.908664Z",
     "start_time": "2021-09-20T16:58:32.895448Z"
    }
   },
   "outputs": [],
   "source": [
    "## Tokenizer : Create dict encodings & vector out of sentences\n",
    "# Remove punctuations & make all lower-case\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer=Tokenizer(num_words=100,oov_token=\"<OOW>\")  # num_words ==> Encode top 100 distinct words\n",
    "tokenizer.fit_on_texts(sentences)          # Encode sentences\n",
    "word_index=tokenizer.word_index            # dictionary of words  (key==>words & value==> index|token)\n",
    "#word_index                         # word dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a618e0aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T17:00:29.602971Z",
     "start_time": "2021-09-20T17:00:29.578620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 3, 5, 1, 1, 1], [4, 2, 3, 6, 1], [7, 2, 3, 5, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Converts Sentences to Vector-Sequence (using word dictionary)\n",
    "sequences=tokenizer.texts_to_sequences(sentences)  \n",
    "sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b6270e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T17:06:40.569228Z",
     "start_time": "2021-09-20T17:06:40.551907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2, 3, 5, 1, 1, 1, 0],\n",
       "       [4, 2, 3, 6, 1, 0, 0, 0],\n",
       "       [7, 2, 3, 5, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Padding vector-sequences: make all Vector-Sequences to same length(by adding 1 @ last)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded=pad_sequences(sequences,maxlen=8,padding=\"post\")\n",
    "padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c6f15a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T17:07:02.801435Z",
     "start_time": "2021-09-20T17:07:02.774999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 4, 2, 3, 5, 1, 1, 1],\n",
       "       [0, 0, 0, 4, 2, 3, 6, 1],\n",
       "       [7, 2, 3, 5, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Padding vector-sequences: make all Vector-Sequences to same length(by adding 1 @ last)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded=pad_sequences(sequences,maxlen=8,padding=\"pre\")\n",
    "padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46536ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eae6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148068bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69eb186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a173b27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T16:44:59.569163Z",
     "start_time": "2021-10-05T16:44:59.170404Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3c7b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T16:49:49.586762Z",
     "start_time": "2021-10-05T16:49:49.562227Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4dc69ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T16:53:59.116055Z",
     "start_time": "2021-10-05T16:53:59.098018Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-18-65e06a191c0a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-65e06a191c0a>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    with open(\"C:\\Users\\JokeRR\\Referances\\python_projects\\Sarcasm data classifier\\Sarcasm_Headlines_Dataset.json\",'r') as f:\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "import json\n",
    "\n",
    "with open(\"C:\\Users\\JokeRR\\Referances\\python_projects\\Sarcasm data classifier\\Sarcasm_Headlines_Dataset.json\",'r') as f:\n",
    "    datastore = json.load(f)\n",
    "    \n",
    "\n",
    "sentences = [] \n",
    "labels = []\n",
    "urls = []\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "print(word_index)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8ea3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa2ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
