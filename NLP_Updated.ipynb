{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "176b6160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\" items, beginning with various punctuation symbols and continuingwith words starting with A. All capitalized words precede lowercase words. We dis-cover the size of the vocabulary indirectly, by asking for the number of items in the set,and again we can use len to obtain this number\"\"\"\n",
    "\n",
    "text_split=text.split(\" \")\n",
    "len(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b60cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad5d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python String Attributes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5283ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cca9da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'A.',\n",
       " 'All',\n",
       " 'We',\n",
       " 'again',\n",
       " 'and',\n",
       " 'asking',\n",
       " 'beginning',\n",
       " 'by',\n",
       " 'can',\n",
       " 'capitalized',\n",
       " 'continuingwith',\n",
       " 'dis-cover',\n",
       " 'for',\n",
       " 'in',\n",
       " 'indirectly,',\n",
       " 'items',\n",
       " 'items,',\n",
       " 'len',\n",
       " 'lowercase',\n",
       " 'number',\n",
       " 'obtain',\n",
       " 'of',\n",
       " 'precede',\n",
       " 'punctuation',\n",
       " 'set,and',\n",
       " 'size',\n",
       " 'starting',\n",
       " 'symbols',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'use',\n",
       " 'various',\n",
       " 'vocabulary',\n",
       " 'we',\n",
       " 'with',\n",
       " 'words',\n",
       " 'words.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(text_split)             # Unique Set\n",
    "sorted(set(text_split))     # Unique Set in sorted\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pandas String Attributes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15584480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be11ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5b5b4d7",
   "metadata": {},
   "source": [
    "## Tokenizing Text :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a14039c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Words Tokenizing : Breaks into word-tokens\n",
    "import nltk\n",
    "word_token=nltk.word_tokenize(text)\n",
    "word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c32f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentence Tokenizing : Breaks into word-tokens\n",
    "import nltk\n",
    "sentence_token=nltk.sent_tokenize(text)\n",
    "sentence_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481ad1c",
   "metadata": {},
   "source": [
    "## Remove Puncuation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75316dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove Punctuation:\n",
    "import string\n",
    "\n",
    "## List of Punctuation \n",
    "string.punctuation        \n",
    "\n",
    "remove_punct=[x for x in word_token if x not in string.punctuation]  ## Remove Punctuation marks\n",
    "remove_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59374c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(txt):\n",
    "    remove_punct=\" \".join([x for x in txt if x not in string.punctuation])\n",
    "    return remove_punct\n",
    "data['crew']=data['crew'].apply(lambda x:remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355fc20",
   "metadata": {},
   "source": [
    "## Removing Stop Words :\n",
    " * ``Common words that generally don't contribute to meaning of sentence``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "## list of Stopword-supported Language\n",
    "stopwords.fileids()  \n",
    "\n",
    "## Specific Language Stop-words:\n",
    "stop_words=stopwords.words(\"english\")\n",
    "stop_words                     # list of 'english' Stop-words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a764857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['items',\n",
       " ',',\n",
       " 'beginning',\n",
       " 'with',\n",
       " 'various',\n",
       " 'punctuation',\n",
       " 'symbols',\n",
       " 'and',\n",
       " 'continuingwith',\n",
       " 'words',\n",
       " 'starting',\n",
       " 'with',\n",
       " 'A',\n",
       " '.',\n",
       " 'All',\n",
       " 'capitalized',\n",
       " 'words',\n",
       " 'precede',\n",
       " 'lowercase',\n",
       " 'words',\n",
       " '.',\n",
       " 'We',\n",
       " 'dis-cover',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'the',\n",
       " 'vocabulary',\n",
       " 'indirectly',\n",
       " ',',\n",
       " 'by',\n",
       " 'asking',\n",
       " 'for',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'items',\n",
       " 'in',\n",
       " 'the',\n",
       " 'set',\n",
       " ',',\n",
       " 'and',\n",
       " 'again',\n",
       " 'we',\n",
       " 'can',\n",
       " 'use',\n",
       " 'len',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'this',\n",
       " 'number']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "## Removing Specific language stopwords\n",
    "word_token=nltk.word_tokenize(text)     # Convert to Word-tokens\n",
    "stop_words=stopwords.words('english')   # Loading stop words for \"English\"\n",
    "Removed_stopwords=[w for w in word_token if w not in stop_words]   # Remove stop words\n",
    "Removed_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f314c63",
   "metadata": {},
   "source": [
    "## Stemming :\n",
    "* `Extract base-form of words` \n",
    "\n",
    "ex: writing | written | wrote -> write\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58efec6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'language'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31036/338143886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# SnowballStemmer: writing -> write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mstrmmerized_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'language'"
     ]
    }
   ],
   "source": [
    "## PorterStemmer : \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "\n",
    "strmmerized_word=[ps.stem(w) for w in word_token]\n",
    "strmmerized_word\n",
    "\n",
    "\n",
    "## LancasterStemmer : \n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc=LancasterStemmer()\n",
    "\n",
    "strmmerized_word=[lc.stem(w) for w in word_token] \n",
    "strmmerized_word\n",
    "\n",
    "\n",
    "## SnowballStemmer :\n",
    "#@ SnowballStemmer(language,ignore_stopwords=False)\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer      \n",
    "sb=SnowballStemmer(\"english\")                   \n",
    "# supported: Arabic,Danish,Dutch,English,Finnish,French,German,Hungarian,Italian,Norwegian,Portuguese,Romanian,Russian,Spanish,Swedish\n",
    "\n",
    "strmmerized_word=[sb.stem(w) for w in word_token] \n",
    "strmmerized_word\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c379e91d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'regexp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31036/3346991551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRegexpStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstrmmerized_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstrmmerized_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'regexp'"
     ]
    }
   ],
   "source": [
    "## RegexpStemmer:\n",
    "#@ RegexpStemmer(regexp,min=0)\n",
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "rs=RegexpStemmer()\n",
    "\n",
    "strmmerized_word=[rs.stem(w) for w in word_token] \n",
    "strmmerized_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f48882",
   "metadata": {},
   "source": [
    "## Lemmatizing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6a37399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item',\n",
       " ',',\n",
       " 'beginning',\n",
       " 'with',\n",
       " 'various',\n",
       " 'punctuation',\n",
       " 'symbol',\n",
       " 'and',\n",
       " 'continuingwith',\n",
       " 'word',\n",
       " 'starting',\n",
       " 'with',\n",
       " 'A',\n",
       " '.',\n",
       " 'All',\n",
       " 'capitalized',\n",
       " 'word',\n",
       " 'precede',\n",
       " 'lowercase',\n",
       " 'word',\n",
       " '.',\n",
       " 'We',\n",
       " 'dis-cover',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'the',\n",
       " 'vocabulary',\n",
       " 'indirectly',\n",
       " ',',\n",
       " 'by',\n",
       " 'asking',\n",
       " 'for',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'item',\n",
       " 'in',\n",
       " 'the',\n",
       " 'set',\n",
       " ',',\n",
       " 'and',\n",
       " 'again',\n",
       " 'we',\n",
       " 'can',\n",
       " 'use',\n",
       " 'len',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'this',\n",
       " 'number']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm=WordNetLemmatizer()\n",
    "\n",
    "lemmatized_word=[lm.lemmatize(w) for w in word_token] \n",
    "lemmatized_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf1387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b191e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Text :\n",
    "strip , split , replace \n",
    "\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "\n",
    "#\n",
    "\n",
    "## Tagging Parts of Speech(Using Pre-trained tagger)\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_tagged=pos_tag(word_tokenize(passage1))  # Use pre-trained part of speech tagger\n",
    "text_tagged   # word & Part of Speech\n",
    "\n",
    "# Filter words according to Parts of Speech\n",
    "tagged=[i for i,tag in text_tagged if tag in['NN','NNS','NNP','NNPS']]\n",
    "tagged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## training own Parts of Speech Tagger\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "# Get some text from the Brown Corpus, broken into sentences\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "# Split into 4000 sentences for training and 623 for testing\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "# Create backoff tagger\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "# Show accuracy\n",
    "trigram.evaluate(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81f672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07812e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee3b34c2",
   "metadata": {},
   "source": [
    "## Bag of Words (Text Encoding) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3aebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentences=nltk.sent_tokenize(passage1)    # word-tokenizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "corpus=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    extracted_word=re.sub('[^a-zA-Z]',' ',sentences[i]) # Remove Punctuations \n",
    "    lower_word=extracted_word.lower()                   # Make Lowercase\n",
    "    split_word=lower_word.split()                       # split into words\n",
    "    extract_stop_word=[lemmatizer.lemmatize(word) for word in split_word if not word in set(stopwords.words('english'))]\n",
    "    join_word=' '.join(extract_stop_word)               # Convert to Sentense\n",
    "    corpus.append(join_word)                            # Append to corpus[]\n",
    "\n",
    "# Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "c_vectorizer=CountVectorizer(max_features=1500)\n",
    "bag_of_words=c_vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "c_vectorizer.get_feature_names()    # Return feature names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704259e",
   "metadata": {},
   "source": [
    "## Weighting Word Importance(Using TFIDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b503abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentences=nltk.sent_tokenize(passage1)    # word-tokenizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "corpus=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    extracted_word=re.sub('[^a-zA-Z]',' ',sentences[i]) # Remove Punctuations \n",
    "    lower_word=extracted_word.lower()                   # Make Lowercase\n",
    "    split_word=lower_word.split()                       # split into words\n",
    "    extract_stop_word=[lemmatizer.lemmatize(word) for word in split_word if not word in set(stopwords.words('english'))]\n",
    "    join_word=' '.join(extract_stop_word)               # Convert to Sentense\n",
    "    corpus.append(join_word)                            # Append to corpus[]\n",
    "    \n",
    "    \n",
    "# TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()\n",
    "weighted_word_importance=cv.fit_transform(corpus).toarray()\n",
    "weighted_word_importance\n",
    "\n",
    "tfidf.vocabulary_    # Return Feature nmaes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75cb49a",
   "metadata": {},
   "source": [
    "## Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "text=re.sub(r'\\[[0-9]*\\]',' ',passage1)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "text=text.lower()\n",
    "text=re.sub(r'\\d',' ',text)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences=nltk.sent_tokenize(text)                                    # to sentences \n",
    "sentences=[nltk.word_tokenize(sentence) for sentence in sentences]    # \n",
    "for i in range(len(sentences)):\n",
    "    sentences[i]=[word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n",
    "# Training the Word2Vec model\n",
    "model=Word2Vec(sentences,min_count=1)\n",
    "words=model.wv.vocab\n",
    "vector=model.wv['learning']              # Finding Word Vectors\n",
    "similar=model.wv.most_similar('neural')  # Most similar words\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085d628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3810jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
