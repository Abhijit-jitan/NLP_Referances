{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "176b6160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\" items, beginning with various punctuation symbols and continuingwith words starting with A. All capitalized words precede lowercase words. We dis-cover the size of the vocabulary indirectly, by asking for the number of items in the set,and again we can use len to obtain this number\"\"\"\n",
    "\n",
    "text_split=text.split(\" \")\n",
    "len(text_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591223a9",
   "metadata": {},
   "source": [
    "# Text pre-processing :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ca2cf",
   "metadata": {},
   "source": [
    "### Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ffb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Words Tokenizing : Breaks into word-tokens\n",
    "import nltk\n",
    "word_token=nltk.word_tokenize(text)\n",
    "word_token\n",
    "\n",
    "\n",
    "## Sentence Tokenizing : Breaks into word-tokens\n",
    "import nltk\n",
    "sentence_token=nltk.sent_tokenize(text)\n",
    "sentence_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beabfe53",
   "metadata": {},
   "source": [
    "### Lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review\"]=data[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cab2bf",
   "metadata": {},
   "source": [
    "### Python String Attributes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee38a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9c3bae3",
   "metadata": {},
   "source": [
    "### Pandas String Attributes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e519230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e5931b4",
   "metadata": {},
   "source": [
    "### Unique words & Sorted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cca9da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T08:14:52.409206Z",
     "start_time": "2021-10-05T08:14:52.369204Z"
    }
   },
   "outputs": [],
   "source": [
    "set(tokenized_word)             # Unique Set\n",
    "sorted(set(tokenized_word))     # Unique Set in sorted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a25f6",
   "metadata": {},
   "source": [
    "### Remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tag(text):\n",
    "    remove_html_tag=re.compile(r'<.*?>')\n",
    "    return re.sub(remove_html_tag,'',text)\n",
    "\n",
    "data['review']=data['review'].apply(remove_html_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481ad1c",
   "metadata": {},
   "source": [
    "## Remove Puncuation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75316dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    review=[letters.lower() for letters in text if letters not in string.punctuation]\n",
    "    review=''.join(review)\n",
    "    return review\n",
    "\n",
    "data['review']=data['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355fc20",
   "metadata": {},
   "source": [
    "## Removing Stop Words :\n",
    " * ``Common words that generally don't contribute to meaning of sentence``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "## list of Stopword-supported Language\n",
    "stopwords.fileids()  \n",
    "\n",
    "## Specific Language Stop-words:\n",
    "stop_words=stopwords.words(\"english\")\n",
    "stop_words  # list of 'english' Stop-words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words=stopwords.words('english')\n",
    "    review=[words for words in sentence.split() if words not in stop_words]\n",
    "    review=' '.join(review)\n",
    "    return review\n",
    "\n",
    "data['review']=data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f314c63",
   "metadata": {},
   "source": [
    "### Stemming :\n",
    "* `Extract base-form of words` \n",
    "\n",
    "ex: writing | written | wrote -> write\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58efec6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'language'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31036/338143886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# SnowballStemmer: writing -> write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mstrmmerized_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'language'"
     ]
    }
   ],
   "source": [
    "## PorterStemmer : \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "\n",
    "strmmerized_word=[ps.stem(w) for w in word_token]\n",
    "strmmerized_word\n",
    "\n",
    "\n",
    "\n",
    "## LancasterStemmer : \n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc=LancasterStemmer()\n",
    "\n",
    "strmmerized_word=[lc.stem(w) for w in word_token] \n",
    "strmmerized_word\n",
    "\n",
    "\n",
    "\n",
    "## SnowballStemmer :\n",
    "#@ SnowballStemmer(language,ignore_stopwords=False)\n",
    "from nltk.stem.snowball import SnowballStemmer      \n",
    "sb=SnowballStemmer(\"english\")                   \n",
    "# supported: Arabic,Danish,Dutch,English,Finnish,French,German,Hungarian,Italian,Norwegian,Portuguese,Romanian,Russian,Spanish,Swedish\n",
    "\n",
    "strmmerized_word=[sb.stem(w) for w in word_token] \n",
    "strmmerized_word\n",
    "\n",
    "\n",
    "\n",
    "## RegexpStemmer:\n",
    "#@ RegexpStemmer(regexp,min=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c856cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer \n",
    "ps=PorterStemmer() \n",
    "\n",
    "data['review']=data['review'].apply(ps.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f48882",
   "metadata": {},
   "source": [
    "### Lemmatizing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a37399a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T08:29:01.286298Z",
     "start_time": "2021-10-05T08:29:01.233253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from nltk.stem import WordNetLemmatizer\\nlm=WordNetLemmatizer()\\n\\nlemmatized_word=[lm.lemmatize(w) for w in word_token] \\nlemmatized_word\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm=WordNetLemmatizer()\n",
    "\n",
    "lemmatized_word=[lm.lemmatize(w) for w in word_token] \n",
    "lemmatized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm=WordNetLemmatizer()\n",
    "\n",
    "data['review']=data['review'].apply(lm.lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c66e1",
   "metadata": {},
   "source": [
    "### Tagging Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b191e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Text :\n",
    "strip , split , replace \n",
    "\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "\n",
    "#\n",
    "\n",
    "## Tagging Parts of Speech(Using Pre-trained tagger)\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_tagged=pos_tag(word_tokenize(passage1))  # Use pre-trained part of speech tagger\n",
    "text_tagged   # word & Part of Speech\n",
    "\n",
    "# Filter words according to Parts of Speech\n",
    "tagged=[i for i,tag in text_tagged if tag in['NN','NNS','NNP','NNPS']]\n",
    "tagged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## training own Parts of Speech Tagger\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "# Get some text from the Brown Corpus, broken into sentences\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "# Split into 4000 sentences for training and 623 for testing\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "# Create backoff tagger\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "# Show accuracy\n",
    "trigram.evaluate(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81f672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbbbdd9f",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b34c2",
   "metadata": {},
   "source": [
    "### Bag of Words (Text Encoding) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3aebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentences=nltk.sent_tokenize(passage1)    # word-tokenizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "corpus=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    extracted_word=re.sub('[^a-zA-Z]',' ',sentences[i]) # Remove Punctuations \n",
    "    lower_word=extracted_word.lower()                   # Make Lowercase\n",
    "    split_word=lower_word.split()                       # split into words\n",
    "    extract_stop_word=[lemmatizer.lemmatize(word) for word in split_word if not word in set(stopwords.words('english'))]\n",
    "    join_word=' '.join(extract_stop_word)               # Convert to Sentense\n",
    "    corpus.append(join_word)                            # Append to corpus[]\n",
    "\n",
    "# Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "c_vectorizer=CountVectorizer(max_features=1500)\n",
    "bag_of_words=c_vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "c_vectorizer.get_feature_names()    # Return feature names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704259e",
   "metadata": {},
   "source": [
    "### Weighting Word Importance(Using TFIDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b503abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentences=nltk.sent_tokenize(passage1)    # word-tokenizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "corpus=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    extracted_word=re.sub('[^a-zA-Z]',' ',sentences[i]) # Remove Punctuations \n",
    "    lower_word=extracted_word.lower()                   # Make Lowercase\n",
    "    split_word=lower_word.split()                       # split into words\n",
    "    extract_stop_word=[lemmatizer.lemmatize(word) for word in split_word if not word in set(stopwords.words('english'))]\n",
    "    join_word=' '.join(extract_stop_word)               # Convert to Sentense\n",
    "    corpus.append(join_word)                            # Append to corpus[]\n",
    "    \n",
    "    \n",
    "# TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()\n",
    "weighted_word_importance=cv.fit_transform(corpus).toarray()\n",
    "weighted_word_importance\n",
    "\n",
    "tfidf.vocabulary_    # Return Feature nmaes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75cb49a",
   "metadata": {},
   "source": [
    "### Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "text=re.sub(r'\\[[0-9]*\\]',' ',passage1)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "text=text.lower()\n",
    "text=re.sub(r'\\d',' ',text)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences=nltk.sent_tokenize(text)                                    # to sentences \n",
    "sentences=[nltk.word_tokenize(sentence) for sentence in sentences]    # \n",
    "for i in range(len(sentences)):\n",
    "    sentences[i]=[word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n",
    "# Training the Word2Vec model\n",
    "model=Word2Vec(sentences,min_count=1)\n",
    "words=model.wv.vocab\n",
    "vector=model.wv['learning']              # Finding Word Vectors\n",
    "similar=model.wv.most_similar('neural')  # Most similar words\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2dd8c7",
   "metadata": {},
   "source": [
    "## With TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efb837d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T17:00:25.241943Z",
     "start_time": "2021-09-20T17:00:25.226700Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'i love my dog so watch out',\n",
    "    'I, love my cat see',\n",
    "    'You love my dog! seen it a while'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91efd858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T16:58:32.908664Z",
     "start_time": "2021-09-20T16:58:32.895448Z"
    }
   },
   "outputs": [],
   "source": [
    "## Tokenizer : Create dict encodings & vector out of sentences\n",
    "# Remove punctuations & make all lower-case\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer=Tokenizer(num_words=100,oov_token=\"<OOW>\")  # num_words ==> Encode top 100 distinct words\n",
    "tokenizer.fit_on_texts(sentences)          # Encode sentences\n",
    "word_index=tokenizer.word_index            # dictionary of words  (key==>words & value==> index|token)\n",
    "#word_index                         # word dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a618e0aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T17:00:29.602971Z",
     "start_time": "2021-09-20T17:00:29.578620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 3, 5, 1, 1, 1], [4, 2, 3, 6, 1], [7, 2, 3, 5, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Converts Sentences to Vector-Sequence (using word dictionary)\n",
    "sequences=tokenizer.texts_to_sequences(sentences)  \n",
    "sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b6270e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T17:06:40.569228Z",
     "start_time": "2021-09-20T17:06:40.551907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2, 3, 5, 1, 1, 1, 0],\n",
       "       [4, 2, 3, 6, 1, 0, 0, 0],\n",
       "       [7, 2, 3, 5, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Padding vector-sequences: make all Vector-Sequences to same length(by adding 1 @ last)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded=pad_sequences(sequences,maxlen=8,padding=\"post\")\n",
    "padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c6f15a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T17:07:02.801435Z",
     "start_time": "2021-09-20T17:07:02.774999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 4, 2, 3, 5, 1, 1, 1],\n",
       "       [0, 0, 0, 4, 2, 3, 6, 1],\n",
       "       [7, 2, 3, 5, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Padding vector-sequences: make all Vector-Sequences to same length(by adding 1 @ last)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded=pad_sequences(sequences,maxlen=8,padding=\"pre\")\n",
    "padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46536ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
