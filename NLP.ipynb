{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedicated-central",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T00:29:45.749384Z",
     "start_time": "2021-04-01T00:29:44.392744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Enter the World of Pandora.\n",
       "1          At the end of the world, the adventure begins.\n",
       "2                                   A Plan No One Escapes\n",
       "3                                         The Legend Ends\n",
       "4                    Lost in our world, found in another.\n",
       "                              ...                        \n",
       "4798    He didn't come looking for trouble, but troubl...\n",
       "4799    A newlywed couple's honeymoon is upended by th...\n",
       "4800                                              Nothing\n",
       "4801                             A New Yorker in Shanghai\n",
       "4802                                              Nothing\n",
       "Name: tagline, Length: 4803, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(r\"E:\\Projects\\movie_dataset.csv\")\n",
    "data.fillna(\"Nothing\",inplace=True)\n",
    "data['tagline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unnecessary-phoenix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T00:29:46.336643Z",
     "start_time": "2021-04-01T00:29:46.307643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             enter the world of pandora.\n",
       "1          at the end of the world, the adventure begins.\n",
       "2                                   a plan no one escapes\n",
       "3                                         the legend ends\n",
       "4                    lost in our world, found in another.\n",
       "                              ...                        \n",
       "4798    he didn't come looking for trouble, but troubl...\n",
       "4799    a newlywed couple's honeymoon is upended by th...\n",
       "4800                                              nothing\n",
       "4801                             a new yorker in shanghai\n",
       "4802                                              nothing\n",
       "Name: formed, Length: 4803, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"formed\"]=data['tagline'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "data[\"formed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "medical-criterion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()   # Package Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "english-museum",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T11:07:33.306642Z",
     "start_time": "2021-04-01T11:07:33.297611Z"
    }
   },
   "outputs": [],
   "source": [
    "text=\"write written writing wrote Tokenization may be defined as the Process of breaking the given text, into smaller units called tokens. Words, numbers or punctuation marks can be tokens. It may also be called word segmentation We have different packages for tokenization provided by NLTK. We can use these packages based on our requirements. The packages and the details of their installation are as follows After installing NLTK, another important task is to download its preset text repositories so that it can be easily used. However, before that we need to import NLTK the way we import any other Python module. The following command will help us in importing NLTK Due to grammatical reasons, language includes lots of variations. Variations in the sense that the language, English as well as other languages too, have different forms of a word. For example, the words like democracy, democratic, and democratization. For machine learning projects, it is very important for machines to understand that these different words, like above, have the same base form. That is why it is very useful to extract the base forms of the words while analyzing the text   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "together-width",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T18:48:49.727534Z",
     "start_time": "2021-03-28T18:48:49.665531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>director</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>237000000</td>\n",
       "      <td>Action Adventure Fantasy Science Fiction</td>\n",
       "      <td>http://www.avatarmovie.com/</td>\n",
       "      <td>19995</td>\n",
       "      <td>culture clash future space war space colony so...</td>\n",
       "      <td>en</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>150.437577</td>\n",
       "      <td>...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Enter the World of Pandora.</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>7.2</td>\n",
       "      <td>11800</td>\n",
       "      <td>Sam Worthington Zoe Saldana Sigourney Weaver S...</td>\n",
       "      <td>[{'name': 'Stephen E. Rivkin', 'gender': 0, 'd...</td>\n",
       "      <td>James Cameron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>300000000</td>\n",
       "      <td>Adventure Fantasy Action</td>\n",
       "      <td>http://disney.go.com/disneypictures/pirates/</td>\n",
       "      <td>285</td>\n",
       "      <td>ocean drug abuse exotic island east india trad...</td>\n",
       "      <td>en</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>Captain Barbossa, long believed to be dead, ha...</td>\n",
       "      <td>139.082615</td>\n",
       "      <td>...</td>\n",
       "      <td>169.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>At the end of the world, the adventure begins.</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>6.9</td>\n",
       "      <td>4500</td>\n",
       "      <td>Johnny Depp Orlando Bloom Keira Knightley Stel...</td>\n",
       "      <td>[{'name': 'Dariusz Wolski', 'gender': 2, 'depa...</td>\n",
       "      <td>Gore Verbinski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>245000000</td>\n",
       "      <td>Action Adventure Crime</td>\n",
       "      <td>http://www.sonypictures.com/movies/spectre/</td>\n",
       "      <td>206647</td>\n",
       "      <td>spy based on novel secret agent sequel mi6</td>\n",
       "      <td>en</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>A cryptic message from Bond’s past sends him o...</td>\n",
       "      <td>107.376788</td>\n",
       "      <td>...</td>\n",
       "      <td>148.0</td>\n",
       "      <td>[{\"iso_639_1\": \"fr\", \"name\": \"Fran\\u00e7ais\"},...</td>\n",
       "      <td>Released</td>\n",
       "      <td>A Plan No One Escapes</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>6.3</td>\n",
       "      <td>4466</td>\n",
       "      <td>Daniel Craig Christoph Waltz L\\u00e9a Seydoux ...</td>\n",
       "      <td>[{'name': 'Thomas Newman', 'gender': 2, 'depar...</td>\n",
       "      <td>Sam Mendes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>250000000</td>\n",
       "      <td>Action Crime Drama Thriller</td>\n",
       "      <td>http://www.thedarkknightrises.com/</td>\n",
       "      <td>49026</td>\n",
       "      <td>dc comics crime fighter terrorist secret ident...</td>\n",
       "      <td>en</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>Following the death of District Attorney Harve...</td>\n",
       "      <td>112.312950</td>\n",
       "      <td>...</td>\n",
       "      <td>165.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>The Legend Ends</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>7.6</td>\n",
       "      <td>9106</td>\n",
       "      <td>Christian Bale Michael Caine Gary Oldman Anne ...</td>\n",
       "      <td>[{'name': 'Hans Zimmer', 'gender': 2, 'departm...</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>260000000</td>\n",
       "      <td>Action Adventure Science Fiction</td>\n",
       "      <td>http://movies.disney.com/john-carter</td>\n",
       "      <td>49529</td>\n",
       "      <td>based on novel mars medallion space travel pri...</td>\n",
       "      <td>en</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>John Carter is a war-weary, former military ca...</td>\n",
       "      <td>43.926995</td>\n",
       "      <td>...</td>\n",
       "      <td>132.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Lost in our world, found in another.</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2124</td>\n",
       "      <td>Taylor Kitsch Lynn Collins Samantha Morton Wil...</td>\n",
       "      <td>[{'name': 'Andrew Stanton', 'gender': 2, 'depa...</td>\n",
       "      <td>Andrew Stanton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     budget                                    genres  \\\n",
       "0      0  237000000  Action Adventure Fantasy Science Fiction   \n",
       "1      1  300000000                  Adventure Fantasy Action   \n",
       "2      2  245000000                    Action Adventure Crime   \n",
       "3      3  250000000               Action Crime Drama Thriller   \n",
       "4      4  260000000          Action Adventure Science Fiction   \n",
       "\n",
       "                                       homepage      id  \\\n",
       "0                   http://www.avatarmovie.com/   19995   \n",
       "1  http://disney.go.com/disneypictures/pirates/     285   \n",
       "2   http://www.sonypictures.com/movies/spectre/  206647   \n",
       "3            http://www.thedarkknightrises.com/   49026   \n",
       "4          http://movies.disney.com/john-carter   49529   \n",
       "\n",
       "                                            keywords original_language  \\\n",
       "0  culture clash future space war space colony so...                en   \n",
       "1  ocean drug abuse exotic island east india trad...                en   \n",
       "2         spy based on novel secret agent sequel mi6                en   \n",
       "3  dc comics crime fighter terrorist secret ident...                en   \n",
       "4  based on novel mars medallion space travel pri...                en   \n",
       "\n",
       "                             original_title  \\\n",
       "0                                    Avatar   \n",
       "1  Pirates of the Caribbean: At World's End   \n",
       "2                                   Spectre   \n",
       "3                     The Dark Knight Rises   \n",
       "4                               John Carter   \n",
       "\n",
       "                                            overview  popularity  ... runtime  \\\n",
       "0  In the 22nd century, a paraplegic Marine is di...  150.437577  ...   162.0   \n",
       "1  Captain Barbossa, long believed to be dead, ha...  139.082615  ...   169.0   \n",
       "2  A cryptic message from Bond’s past sends him o...  107.376788  ...   148.0   \n",
       "3  Following the death of District Attorney Harve...  112.312950  ...   165.0   \n",
       "4  John Carter is a war-weary, former military ca...   43.926995  ...   132.0   \n",
       "\n",
       "                                    spoken_languages    status  \\\n",
       "0  [{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...  Released   \n",
       "1           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]  Released   \n",
       "2  [{\"iso_639_1\": \"fr\", \"name\": \"Fran\\u00e7ais\"},...  Released   \n",
       "3           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]  Released   \n",
       "4           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]  Released   \n",
       "\n",
       "                                          tagline  \\\n",
       "0                     Enter the World of Pandora.   \n",
       "1  At the end of the world, the adventure begins.   \n",
       "2                           A Plan No One Escapes   \n",
       "3                                 The Legend Ends   \n",
       "4            Lost in our world, found in another.   \n",
       "\n",
       "                                      title vote_average vote_count  \\\n",
       "0                                    Avatar          7.2      11800   \n",
       "1  Pirates of the Caribbean: At World's End          6.9       4500   \n",
       "2                                   Spectre          6.3       4466   \n",
       "3                     The Dark Knight Rises          7.6       9106   \n",
       "4                               John Carter          6.1       2124   \n",
       "\n",
       "                                                cast  \\\n",
       "0  Sam Worthington Zoe Saldana Sigourney Weaver S...   \n",
       "1  Johnny Depp Orlando Bloom Keira Knightley Stel...   \n",
       "2  Daniel Craig Christoph Waltz L\\u00e9a Seydoux ...   \n",
       "3  Christian Bale Michael Caine Gary Oldman Anne ...   \n",
       "4  Taylor Kitsch Lynn Collins Samantha Morton Wil...   \n",
       "\n",
       "                                                crew           director  \n",
       "0  [{'name': 'Stephen E. Rivkin', 'gender': 0, 'd...      James Cameron  \n",
       "1  [{'name': 'Dariusz Wolski', 'gender': 2, 'depa...     Gore Verbinski  \n",
       "2  [{'name': 'Thomas Newman', 'gender': 2, 'depar...         Sam Mendes  \n",
       "3  [{'name': 'Hans Zimmer', 'gender': 2, 'departm...  Christopher Nolan  \n",
       "4  [{'name': 'Andrew Stanton', 'gender': 2, 'depa...     Andrew Stanton  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-swift",
   "metadata": {},
   "source": [
    "# Data Extraction: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-qualification",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization : Breaking texts into words or sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenizer:\n",
    "\n",
    "from nltk.tokenize import sent_tokenize     # Sentence Tokenizer\n",
    "sentences=sent_tokenize(text)  # List of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "compound-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenizer:\n",
    "\n",
    "from nltk.tokenize import word_tokenize     # Word Tokenizer\n",
    "words=word_tokenize(text)   # List of words\n",
    "\n",
    "or\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer  # tree bank word-tokenizer\n",
    "words=TreebankWordTokenizer(text)\n",
    "\n",
    "or \n",
    "\n",
    "from nltk.tokenize import PunktWordTokenizer  # Punkt Word-Tokenizer\n",
    "words=PunktWordTokenizer(text)\n",
    "\n",
    "or\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer  # WordPunct Tokenizer\n",
    "words=WordPunctTokenizer(text)\n",
    "\n",
    "or\n",
    "\n",
    "tokenizer=RegexpTokenizer('\\s+',gaps=True)  # Simple whitespace tokenizer\n",
    "words=tokenizer.tokenize(text)\n",
    "\n",
    "or\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imposed-software",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T10:45:05.837690Z",
     "start_time": "2021-04-01T10:45:05.821657Z"
    }
   },
   "outputs": [],
   "source": [
    "## join words to entences\n",
    "remove_punct=\" \".join([x for x in word_token if x not in string.punctuation])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-shoot",
   "metadata": {},
   "source": [
    "#  Parts of Speech Tagging :\n",
    "**Common PoS :** <br> \n",
    " - Noun : n <br>\n",
    " - Adjective : a <br>\n",
    " - Adverb : r <br>\n",
    " - Verb : v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "significant-strategy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T12:47:55.995873Z",
     "start_time": "2021-03-27T12:47:55.972871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet.synsets('cook'))          # Total PoS\n",
    "\n",
    "\n",
    "len(wordnet.synsets('cook',pos='n'))\n",
    "len(wordnet.synsets('cook',pos='a'))\n",
    "len(wordnet.synsets('cook',pos='r'))\n",
    "len(wordnet.synsets('cook',pos='v'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "miniature-dealer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('write', 'RB'),\n",
       " ('written', 'VBN'),\n",
       " ('writing', 'VBG'),\n",
       " ('wrote', 'VBD'),\n",
       " ('Tokenization', 'NNP'),\n",
       " ('may', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('defined', 'VBN'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Process', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('breaking', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('given', 'VBN'),\n",
       " ('text', 'NN'),\n",
       " (',', ','),\n",
       " ('into', 'IN'),\n",
       " ('smaller', 'JJR'),\n",
       " ('units', 'NNS'),\n",
       " ('called', 'VBD'),\n",
       " ('tokens', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Words', 'NNS'),\n",
       " (',', ','),\n",
       " ('numbers', 'NNS'),\n",
       " ('or', 'CC'),\n",
       " ('punctuation', 'NN'),\n",
       " ('marks', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('tokens', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('may', 'MD'),\n",
       " ('also', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('called', 'VBN'),\n",
       " ('word', 'NN'),\n",
       " ('segmentation', 'NN'),\n",
       " ('We', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('different', 'JJ'),\n",
       " ('packages', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('tokenization', 'NN'),\n",
       " ('provided', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('use', 'VB'),\n",
       " ('these', 'DT'),\n",
       " ('packages', 'NNS'),\n",
       " ('based', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('requirements', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('packages', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('details', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('installation', 'NN'),\n",
       " ('are', 'VBP'),\n",
       " ('as', 'IN'),\n",
       " ('follows', 'VBZ'),\n",
       " ('After', 'IN'),\n",
       " ('installing', 'VBG'),\n",
       " ('NLTK', 'NNP'),\n",
       " (',', ','),\n",
       " ('another', 'DT'),\n",
       " ('important', 'JJ'),\n",
       " ('task', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('download', 'VB'),\n",
       " ('its', 'PRP$'),\n",
       " ('preset', 'NN'),\n",
       " ('text', 'NN'),\n",
       " ('repositories', 'NNS'),\n",
       " ('so', 'IN'),\n",
       " ('that', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('easily', 'RB'),\n",
       " ('used', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('However', 'RB'),\n",
       " (',', ','),\n",
       " ('before', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('we', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('import', 'VB'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('the', 'DT'),\n",
       " ('way', 'NN'),\n",
       " ('we', 'PRP'),\n",
       " ('import', 'VBP'),\n",
       " ('any', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " ('Python', 'NNP'),\n",
       " ('module', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('following', 'JJ'),\n",
       " ('command', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('help', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('in', 'IN'),\n",
       " ('importing', 'VBG'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('Due', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('grammatical', 'JJ'),\n",
       " ('reasons', 'NNS'),\n",
       " (',', ','),\n",
       " ('language', 'NN'),\n",
       " ('includes', 'VBZ'),\n",
       " ('lots', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('variations', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Variations', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('sense', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('language', 'NN'),\n",
       " (',', ','),\n",
       " ('English', 'NNP'),\n",
       " ('as', 'RB'),\n",
       " ('well', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('other', 'JJ'),\n",
       " ('languages', 'NNS'),\n",
       " ('too', 'RB'),\n",
       " (',', ','),\n",
       " ('have', 'VBP'),\n",
       " ('different', 'JJ'),\n",
       " ('forms', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('word', 'NN'),\n",
       " ('.', '.'),\n",
       " ('For', 'IN'),\n",
       " ('example', 'NN'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('words', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('democracy', 'NN'),\n",
       " (',', ','),\n",
       " ('democratic', 'JJ'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('democratization', 'NN'),\n",
       " ('.', '.'),\n",
       " ('For', 'IN'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'NN'),\n",
       " ('projects', 'NNS'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('very', 'RB'),\n",
       " ('important', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('machines', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('understand', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('these', 'DT'),\n",
       " ('different', 'JJ'),\n",
       " ('words', 'NNS'),\n",
       " (',', ','),\n",
       " ('like', 'IN'),\n",
       " ('above', 'IN'),\n",
       " (',', ','),\n",
       " ('have', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('same', 'JJ'),\n",
       " ('base', 'NN'),\n",
       " ('form', 'NN'),\n",
       " ('.', '.'),\n",
       " ('That', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('why', 'WRB'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('very', 'RB'),\n",
       " ('useful', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('extract', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('base', 'NN'),\n",
       " ('forms', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('words', 'NNS'),\n",
       " ('while', 'IN'),\n",
       " ('analyzing', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('text', 'NN')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "tagged_text=pos_tag(words)\n",
    "tagged_text\n",
    "\n",
    "# Filter words\n",
    "filtered_pos=[word for word,tag in tagged_text if tag in ['NN','NNS','NNP','NNPS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-contemporary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T12:04:19.650159Z",
     "start_time": "2021-03-27T12:04:19.631038Z"
    }
   },
   "source": [
    "# Looking up Synsets :\n",
    "\n",
    "**Synset :** words that express \"same concept\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "floppy-rehabilitation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T12:59:44.609729Z",
     "start_time": "2021-03-27T12:59:44.591738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn=wordnet.synsets('cooking')[0]\n",
    "syn.name()         # Synsets Name\n",
    "syn.definition()   # Synset Definations\n",
    "\n",
    "wordnet.synsets('cooking')[0].examples()  # example Sentences\n",
    "\n",
    "lemmas=syn.lemmas()  # conical form of word\n",
    "lemmas\n",
    "\n",
    "\n",
    "\n",
    "# word-net Synset Similarrity :\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "wordnet.synset('cookbook.n.01').wup_similarity(wordnet.synset('instruction_book.n.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-novel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T12:10:47.001097Z",
     "start_time": "2021-03-27T12:10:46.981337Z"
    }
   },
   "source": [
    "# Working With hypernyms :\n",
    "\n",
    "**hypernyms :** Synsets are organized in a structure similar to that of an inheritance tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "treated-factor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T12:38:16.451695Z",
     "start_time": "2021-03-27T12:38:16.433641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('abstraction.n.06'),\n",
       "  Synset('psychological_feature.n.01'),\n",
       "  Synset('event.n.01'),\n",
       "  Synset('act.n.02'),\n",
       "  Synset('action.n.01'),\n",
       "  Synset('change.n.03'),\n",
       "  Synset('change_of_state.n.01'),\n",
       "  Synset('cooking.n.01')]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn=wordnet.synsets('cooking')[0]\n",
    "syn.hypernyms()   \n",
    "\n",
    "syn.hypernyms()[0].hyponyms()\n",
    "\n",
    "syn.root_hypernyms()\n",
    "\n",
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spelling Correction :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-looking",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "## Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-algebra",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-indiana",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "Find Synonyms From NLTK WordNet\n",
    "WordNet is an NLP database with synonyms, antonyms, and brief definitions. We downloaded this with the NLTK downloader\n",
    ">>> from nltk.corpus import wordnet\n",
    ">>> syn=wordnet.synsets('love')\n",
    ">>> syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "imported-bobby",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T11:22:26.298077Z",
     "start_time": "2021-04-01T11:22:26.257037Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create Feature : Message Length\n",
    "data[\"msg_length\"]=data[\"crew\"].apply(lambda x:len(x))\n",
    "\n",
    "\n",
    "## Create Feature: Used Punctuation percentage \n",
    "import string\n",
    "def punct_cnt(txt):\n",
    "    count=sum((1 for x in txt if x in string.punctuation))\n",
    "    return 100*count/len(txt)    # punctuation Percentage\n",
    "\n",
    "data[\"punctuation_%\"]=data[\"tagline\"].apply(lambda x:punct_cnt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-fairy",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "## Bag of Words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "scheduled-ending",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "sentences=nltk.sent_tokenize(text)  # Sentence Tokenizer\n",
    "\n",
    "prepared_Sentences=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    puncuation_removed=re.sub('[^a-zA-Z]',' ',sentences[i])  # Remove Puncuation\n",
    "    lower_sent=puncuation_removed.lower()                    # Make In Lower_case\n",
    "    split_sent=lower_sent.split()                            # Split Sentences to Words\n",
    "    # Perform Stemming & Remove Stop-Words\n",
    "    stem_sent=[ps.stem(word) for word in split_sent if not word in set(stopwords.words('english'))]\n",
    "    joined_sent=' '.join(stem_sent)                             # Join words into Sentences\n",
    "    prepared_Sentences.append(joined_sent)\n",
    "    \n",
    "## Perform \"Bag of Words\" model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=1500)\n",
    "x=cv.fit_transform(prepared_Sentences).toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-haven",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pleasant-purchase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T11:08:34.287922Z",
     "start_time": "2021-04-01T11:08:34.179687Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['also',\n",
       " 'analyz',\n",
       " 'anoth',\n",
       " 'base',\n",
       " 'break',\n",
       " 'call',\n",
       " 'command',\n",
       " 'defin',\n",
       " 'democraci',\n",
       " 'democrat',\n",
       " 'detail',\n",
       " 'differ',\n",
       " 'download',\n",
       " 'due',\n",
       " 'easili',\n",
       " 'english',\n",
       " 'exampl',\n",
       " 'extract',\n",
       " 'follow',\n",
       " 'form',\n",
       " 'given',\n",
       " 'grammat',\n",
       " 'help',\n",
       " 'howev',\n",
       " 'import',\n",
       " 'includ',\n",
       " 'instal',\n",
       " 'languag',\n",
       " 'learn',\n",
       " 'like',\n",
       " 'lot',\n",
       " 'machin',\n",
       " 'mark',\n",
       " 'may',\n",
       " 'modul',\n",
       " 'need',\n",
       " 'nltk',\n",
       " 'number',\n",
       " 'packag',\n",
       " 'preset',\n",
       " 'process',\n",
       " 'project',\n",
       " 'provid',\n",
       " 'punctuat',\n",
       " 'python',\n",
       " 'reason',\n",
       " 'repositori',\n",
       " 'requir',\n",
       " 'segment',\n",
       " 'sens',\n",
       " 'smaller',\n",
       " 'task',\n",
       " 'text',\n",
       " 'token',\n",
       " 'understand',\n",
       " 'unit',\n",
       " 'us',\n",
       " 'use',\n",
       " 'variat',\n",
       " 'way',\n",
       " 'well',\n",
       " 'word',\n",
       " 'write',\n",
       " 'written',\n",
       " 'wrote']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re,nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "sentences=nltk.sent_tokenize(text)  # Sentence Tokenizer\n",
    "\n",
    "prepared_Sentences=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    puncuation_removed=re.sub('[^a-zA-Z]',' ',sentences[i])  # Remove Puncuation\n",
    "    lower_sent=puncuation_removed.lower()                    # Make In Lower_case\n",
    "    split_sent=lower_sent.split()                            # Split Sentences to Words\n",
    "    # Perform Stemming & Remove Stop-Words\n",
    "    stem_sent=[ps.stem(word) for word in split_sent if not word in set(stopwords.words('english'))]\n",
    "    joined_sent=' '.join(stem_sent)                             # Join words into Sentences\n",
    "    prepared_Sentences.append(joined_sent)\n",
    " \n",
    "    \n",
    "# Creating the TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()\n",
    "x=tfidf.fit_transform(prepared_Sentences).toarray()\n",
    "\n",
    "columns=tfidf.get_feature_names()\n",
    "columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "superior-advertiser",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'write' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-d6ae8a2a0d4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Finding Word Vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mvector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'write'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Most similar words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joker\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joker\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joker\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'write' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "## Word2Vec Model :\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "sentences=nltk.sent_tokenize(text)  # Sentence Tokenizer\n",
    "\n",
    "prepared_Sentences=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    puncuation_removed=re.sub('[^a-zA-Z]',' ',sentences[i])  # Remove Puncuation\n",
    "    lower_sent=puncuation_removed.lower()                    # Make In Lower_case\n",
    "    split_sent=lower_sent.split()                            # Split Sentences to Words\n",
    "    # Perform Stemming & Remove Stop-Words\n",
    "    stem_sent=[ps.stem(word) for word in split_sent if not word in set(stopwords.words('english'))]\n",
    "    joined_sent=' '.join(stem_sent)                             # Join words into Sentences\n",
    "    prepared_Sentences.append(joined_sent)\n",
    "\n",
    "# Train Word2Vec:\n",
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec(prepared_Sentences,min_count=1)\n",
    "\n",
    "\n",
    "words=model.wv.vocab\n",
    "\n",
    "# Finding Word Vectors\n",
    "vector=model.wv['write']\n",
    "\n",
    "# Most similar words\n",
    "similar=model.wv.most_similar('vikram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "# Training the Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-calvin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-registrar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-bikini",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-print",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-assets",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-condition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-majority",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-cleveland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-gauge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "stemmer = PorterStemmer()\n",
    "# Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)   \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
